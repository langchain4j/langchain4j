This is an initial / draft implementation of https://github.com/langchain4j/langchain4j/issues/2669, "[FEATURE] Add support for LLM inference via ONNX Runtime #2669".

The module here provides a simple integration with https://github.com/microsoft/onnxruntime-genai, which is a library for running generative AI with ONNX Runtime and including an early access Java API (https://onnxruntime.ai/docs/genai/api/java.html).

As of August 2025, the Java APIs for onnxruntime-genai are not deployed to Maven Central (but the feature is in the roadmap), so you need to manually build onnxruntime-genai 0.9.0 or greater from sources.
The current documentation for building onnxruntime-genai is available at https://onnxruntime.ai/docs/genai/howto/build-from-source.html ; in any case, the suggested command is: 'python build.py --config Release --build_java --parallel --update --build --test --publish_java_maven_local' 

The testsuite relies on a ONNX version of Llama 3.2 1B, which has been obtained using src/python/py/models/builder.py script from the onnxruntime-genai sources; documentation on the tool available at https://github.com/microsoft/onnxruntime-genai/blob/main/src/python/py/models/README.md
