---
sidebar_position: 3
---

# 1. Language and Chat Models

LLMs are currently available in two API types:
- `LanguageModel`s. Their API is very simple - they accept a `String` as input and return a `String` as output. 
Initially, they were trained for text completion, and later for following instructions.
Now, they are becoming obsolete in favor of chat models.
- `ChatLanguageModel`s. These accept either a single or multiple `ChatMessage`s as input
and return an `AiMessage` as output. Some of them also support accepting both `String`s and `Image`s as user input.
They are trained for a variety of tasks and are much more versatile.
Examples of such chat models include OpenAI's `gpt-3.5-turbo` and Google's `gemini-pro`.

Support for `LanguageModel`s will no longer be expanded in LangChain4j,
so in all new features, we will use a `ChatLanguageModel` API.

Now, let's take a closer look at the `ChatLanguageModel` API.

```java
public interface ChatLanguageModel {

    String generate(String userMessage);
    
    ...
}
```
As you can see, there is a `generate` method that takes a `String` as input and returns a `String` as output, similar to `LanguageModel`.
This is a convenience method so you can play around quickly and easily without needing to wrap the `String` in a `UserMessage`.

Let's continue:
```java
    ...
    
    Response<AiMessage> generate(ChatMessage... messages);

    Response<AiMessage> generate(List<ChatMessage> messages);
        
    ...
```

These versions of the `generate` methods take one or multiple `ChatMessage`s as input.
`ChatMessage` is a base interface that represents a chat message.

There are currently four implementations, one for each "source" of the message:

- `UserMessage`: This is a message from the user.
The user can be either an end user of your application (a human) or your application itself.
Depending on the modalities supported by the LLM, `UserMessage` can contain either just text (`String)`, or text and/or images (`Image`).
- `AiMessage`: This is a message that was generated by the AI.
As you might have noted, the generate method returns an `AiMessage` wrapped in a `Response`.
`AiMessage` can contain either a textual response (`String`), or a request to execute a tool (`ToolExecutionRequest`).
Don't worry, we will explore tools a bit later.
- `ToolExecutionResultMessage`: This is the result of the `ToolExecutionRequest`. We will cover this more a bit later.
- `SystemMessage`: This is a message from the system.
Usually, you, as a developer, should define the content of this message.
Normally, you would write here instructions on what the LLM's role is in this conversation,
how it should behave, in what style to answer, etc.
LLMs are trained to pay more attention to `SystemMessage` than to other types of messages,
so be careful and it's better not to give an end user free access to define or inject some input into a `SystemMessage`.
Usually, it is located at the start of the conversation.

Now that we know all types of `ChatMessage`, let's see how we can combine them in the conversation.

The easiest option is to provide a single instance of `UserMessage` into the `generate` method.
This is similar to the first version of the `generate` method, which takes a `String` as input.
The major difference here is that it now returns not a `String`, but a `Response<AiMessage>`.
`Response` is a wrapper around content (payload), and you will often see it as a return type of `*Model` classes.
In addition to content (in this case, `AiMessage`), `Response` also contains meta information about the generation.
First, it includes `TokenUsage`, which contains stats about how many tokens the input
(all the `ChatMessages` that you provided to the generate method) contained,
how many tokens were generated as output (in the `AiMessage`), and the total (input + output).
You will need this information to calculate how much a given call to the LLM costs.
Then, `Response` also contains `FinishReason`, which is an enum with various reasons why generation has stopped.
Usually, it will be `FinishReason.STOP`, if the LLM decided to stop generation itself.

There are multiple ways to create a `UserMessage`, depending on the contents.
The easiest one is `new UserMessage("Hi")` or `UserMessage.from("Hi")`.

TODO

### 'Hello World' Code

Create a class and add the following code

```java
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.openai.OpenAiChatModel;

public class HelloWorld {
    public static void main(String[] args) {
        ChatLanguageModel model = OpenAiChatModel.withApiKey(ApiKeys.OPENAI_API_KEY);

        String response = model.generate("Say 'Hello World'");
        System.out.println(response);

        String response2 = model.generate("What did I just ask?");
        System.out.println(response2);
    }
}
```

Running the program will generate a variant of the following output

<!-- TODO console log formatting? -->
```plaintext
Hello World! How can I assist you today?

You just asked, "what did I just ask you?"
```

### Explanation
We first set up a connection to our LLM, in this case we choose a model from OpenAI (```OpenAIChatModel```).
To connect to OpenAI's models via API, we need a key, that is taken from ```ApiKeys.OPENAI_API_KEY```.

```java
ChatLanguageModel model = OpenAiChatModel.withApiKey(ApiKeys.OPENAI_API_KEY);
```

Next, we interact with the model using the ```generate``` method:

```java
String response = model.generate("Say 'Hello World'");
```

This will return a String with the LLM's answer. 
Note that the API itself does not retain your former prompts. In [Chat (Memory)](chat) you will learn how to pass along your chat history, so the LLM knows what has been said before. If you don't pass the chat history, like in this simple example, the LLM will not be able to correctly answer the second question ('What did I just ask?').

A lot of parameters are set behind the scenes, such as timeout, model type and model parameters.
In [Set Model Parameters](set-model-parameters) you will learn how to set these parameters explicitly.

If you want to use another LLM, langchain4j has integrations with all major model providers (Google Vertex and Gemini, Azure OpenAI, ...) and (local) model wrappers (HuggingFace, Ollama, LocalAI).
All integrations with examples and tutorials are listed under [Integrations](/docs/category/integrations).

In this easy example, the output is returned as a complete String. If you want to print the output token by token, use [Streaming](response-streaming).